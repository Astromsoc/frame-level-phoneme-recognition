# sample configuration file

# data loading parameters -- stay unchanged for one fixed mission
PHONEME_TXT_FILEPATH: "data/phonemes.txt"
TRAIN_DATA_DIR: "data/train-clean-100" 
DEV_DATA_DIR: "data/dev-clean" 


# configurables
context_len: 40
device_num_workers: 8

model:
  add_powers: 2
  first_layer_activation: "softplus"
  linear:
    - 2048
    - 1024
    - 1024
    - 1024
    - 512
  activation:
    - "leakyrelu"
    - "leakyrelu"
    - "leakyrelu"
    - "softplus"
    - "softplus"
  dropout:
    - 0.12
    - 0.12
    - 0.11
    - 0.11
    - 0.10
  batchnorm:
    - true
    - true
    - true
    - true
    - true
  init: "kaiming"

optimizer:
  name: "AdamW"
  configs:
    lr: 1.0e-3

# scheduler: comment the entire section if unwanted
scheduler:
  name: cosine_annealing_warm_restarts
  configs:
    # since which batch (in total) to warm up
    T_0: 40000

training:
  seed: 11785
  epochs: 10
  batch_size: 10000
  # how many evaluations per epoch
  eval_per_epoch: 5
  # whether to add noise for inputs during training
  # ** switch to false if unwanted, but don't comment **
  add_noise: true
  # ** the following 2 flags can be commented **
  # checkpoint for the start of training -- if needed
  init_checkpoint: "**your-checkpoint-filepath-here**"
  # whether to load optimizer [CURRENTLY FACING MULTI-DEVICE PROB]
  load_optimizer_checkpoint: false

inference:
  batch_size: 10000
