# sample configuration file

# data loading parameters -- stay unchanged for one fixed mission
PHONEME_TXT_FILEPATH: "data/phonemes.txt"
TRAIN_DATA_DIR: "data/train-clean-100" 
DEV_DATA_DIR: "data/dev-clean" 


# configurables
context_len: 50
device_num_workers: 8

model:
  add_powers: 2
  first_layer_activation: "softplus"
  linear:
    - 2048
    - 1024
    - 1024
    - 1024
    - 1024
    - 512
  activation:
    - "softplus"
    - "softplus"
    - "softplus"
    - "softplus"
    - "softplus"
    - "softplus"
  dropout:
    - 0.12
    - 0.12
    - 0.12
    - 0.12
    - 0.12
    - 0.12
  batchnorm:
    - true
    - true
    - true
    - true
    - true
    - true
  init: "kaiming"

optimizer:
  # choose from:
  # 'adam', 'adamw', 'adagrad', 'nadam', 'sgd'
  name: "adamw"
  configs:
    lr: 0.00001

# scheduler: comment the entire section if unwanted
scheduler:
  # choose from:
  # 'one_cycle_lr', 'reduce_lr_on_plateau', 'exponential_lr'
  # 'cosine_annealing_lr', 'cosine_annealing_warm_restarts'
  # 'multiplicative_lr', 'lambda_lr', 'cyclic_lr'
  name: cyclic_lr
  configs:
    base_lr: 1.0e-7
    max_lr: 1.0e-5
    step_size_up: 1810
    gamma: 0.9

training:
  seed: 11785
  epochs: 10
  batch_size: 10000
  # how many evaluations per epoch
  eval_per_epoch: 2
  # whether to add noise for inputs during training
  # ** switch to false if unwanted, but don't comment **
  noise_level: 0.01
  # ** the following 2 flags can be commented **
  # checkpoint for the start of training -- if needed
  init_checkpoint: "**your-checkpoint-filepath-here**"
  # whether to load optimizer [CURRENTLY FACING MULTI-DEVICE PROB]
  load_optimizer_checkpoint: false

inference:
  batch_size: 10000
