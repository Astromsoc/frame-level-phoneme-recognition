# sample configuration file

# data loading parameters -- stay unchanged for one fixed mission
PHONEME_TXT_FILEPATH: "data/phonemes.txt"
TRAIN_DATA_DIR: "data/train-clean-100" 
DEV_DATA_DIR: "data/dev-clean" 


# configurables
context_len: 25
device_num_workers: 8

model:
  add_powers: 3
  first_layer_activation: "gelu"
  linear:
    - 2048
    - 2048
    - 2048
    - 1024
    - 1024
    - 1024
  activation:
    - "gelu"
    - "gelu"
    - "gelu"
    - "gelu"
    - "gelu"
    - "gelu"
  dropout:
    - 0.40
    - 0.40
    - 0.40
    - 0.30
    - 0.30
    - 0.30
  batchnorm:
    - true
    - true
    - true
    - true
    - true
    - true
  init: "xavier"


optimizer:
  # adam_warmup_epochs: 1
  # restart_optim_interval: 3
  # choose from:
  # 'adam', 'adamw', 'adagrad', 'nadam', 'sgd'
  name: "adamw"
  configs:
    lr: 0.001
    weight_decay: 0.05
    amsgrad: true

  # name: "sgd"
  # configs:
  #   lr: 0.0001
  #   momentum: 0.8
  #   nesterov: true


# scheduler: comment the entire section if unwanted
scheduler:
  # choose from:
  # 'one_cycle_lr', 'reduce_lr_on_plateau', 'exponential_lr'
  # 'cosine_annealing_lr', 'cosine_annealing_warm_restarts'
  # 'multiplicative_lr', 'lambda_lr', 'cyclic_lr'

  # # COSINE_ANNEALING_WARM_RESTARTS
  # name: cosine_annealing_warm_restarts
  # configs:
  #   T_0: 300
  #   T_mult: 2
  #   eta_min: 1.0e-8

  # # EXPONENTIAL_LR
  # name: exponential_lr
  # configs:
  #   gamma: 0.8

  # MULTISTEP
  name: multistep_lr
  configs:
    gamma: 0.5
    milestones:
      - 5
      - 10
      - 15
  

training:
  seed: 11785
  epochs: 20
  batch_size: 5000
  # how many evaluations per epoch
  eval_per_epoch: 3
  # whether to add noise for inputs during training
  # ** switch to false if unwanted, but don't comment **
  noise_level: 0.05
  # ** the following 3 flags can be commented **
  # checkpoint for the start of training -- if needed
  init_checkpoint: "**your-checkpoint-here**"
  # whether to load optimizer [CURRENTLY FACING MULTI-DEVICE PROB]
  load_optimizer_checkpoint: false
  # update dropout rate if necessary
  new_dropout_rates:
    - 0.40
    - 0.40
    - 0.40
    - 0.30
    - 0.30
    - 0.30

    
inference:
  batch_size: 5000
