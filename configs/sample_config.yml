# sample configuration file

# data loading parameters -- stay unchanged for one fixed mission
PHONEME_TXT_FILEPATH: "data/phonemes.txt"
TRAIN_DATA_DIR: "data/train-clean-100" 
DEV_DATA_DIR: "data/dev-clean" 


# configurables
context_len: 50
device_num_workers: 8

model:
  add_powers: 2
  first_layer_activation: "softplus"
  linear:
    - 2048
    - 1024
    - 1024
    - 1024
    - 1024
  activation:
    - "softplus"
    - "softplus"
    - "softplus"
    - "softplus"
  dropout:
    - 0.12
    - 0.12
    - 0.12
    - 0.12
    - 0.12
  batchnorm:
    - true
    - true
    - true
    - true
    - true
  init: "kaiming"

optimizer:
  # choose from:
  # 'adam', 'adamw', 'adagrad', 'nadam', 'sgd'
  name: "adamw"
  configs:
    lr: 1.0e-3

# scheduler: comment the entire section if unwanted
scheduler:
  # choose from:
  # 'one_cycle_lr', 'reduce_lr_on_plateau', 'exponential_lr'
  # 'cosine_annealing_lr', 'cosine_annealing_warm_restarts'
  name: cosine_annealing_warm_restarts
  configs:
    # since which batch (in total) to warm up
    T_0: 500

training:
  seed: 11785
  epochs: 25
  batch_size: 10000
  # how many evaluations per epoch
  eval_per_epoch: 5
  # whether to add noise for inputs during training
  # ** switch to false if unwanted, but don't comment **
  noise_level: 0.001
  # # ** the following 2 flags can be commented **
  # # checkpoint for the start of training -- if needed
  # init_checkpoint: "**your-checkpoint-filepath-here**"
  # # whether to load optimizer [CURRENTLY FACING MULTI-DEVICE PROB]
  # load_optimizer_checkpoint: false

inference:
  batch_size: 10000
